{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0) What: Test steps 1,2,3 in local Postgres instance\n",
    "#Who: Bryan/Jesse\n",
    "\n",
    "#1) What: Create a second table(s) within Postgres. I suggest just adding \"_cleaned\" to the name and use the same prefix. \n",
    "#The schema will remain the same. William - any objections here? We don't have a dev/staging Postgres instance, do we? \n",
    "#Who: Jesse\n",
    "\n",
    "#2a) What: Take Bryan's new Python and test each 'rule,' adding logic for writing to Postgres/the new 'cleaned' table.\n",
    "#Who: Bryan/Jesse\n",
    "\n",
    "#2b) What: Add logic for alerts (SendGrid); fire alert when issue found, correction made.\n",
    "#Who: Jesse\n",
    "\n",
    "#3) What: Verify that results look good in *_cleaned table(s).\n",
    "#Who: Bryan/Jesse/William(?)/anyone else who wants to help test\n",
    "\n",
    "#4) What: Set up cron job to run, initially, every 4-6 hours. Review results and make sure that:\n",
    "#Data is not thrown out, unnecessarily altered/integrity is retained\n",
    "#Any data corrections are done properly\n",
    "#Review / tweak anything else (see Bryan's notes in Asana: \"Data Cleaning Notes\" under \"Data Science Brainstorming\")\n",
    "#Who: Bryan/Jesse\n",
    "\n",
    "#5) What: After 5-7 days of testing, 'deploy.' Cron job does not need to run as often; decide on interval. \n",
    "#Who: Jesse\n",
    "\n",
    "#6) What: Point all relevant queries to *_cleaned table(s)\n",
    "#Who: Bryan/Jesse/Emily\n",
    "\n",
    "\n",
    "#Will it tell the difference between an account that is consistently down or inconstently down.\n",
    "#After missing a reading, does a sensor ever get back to 100% (visualize this)\n",
    "#csv of sensor ids, date, and percentage (goes to a website?)\n",
    "#can it pull the peron's first and last names \n",
    "#first name last name (sensor id), dates, percentages below\n",
    "\n",
    "#Fix tenant names\n",
    "\n",
    "\n",
    "#Code to visualize stuff - for time periods for month and years\n",
    "\n",
    "#Emily source code visualization plot %\n",
    "#Talk to william or jesse about automating and letting noelle access this\n",
    "#clean dat function (remove bad data)\n",
    "#stats and viz stuff - 311 data, hpd data (blog posts)\n",
    "#is hpd monitoring itself at resolving heat complaints?\n",
    "\n",
    "#hpd conversion rate low, who and why? (conversaion rate = complaint to investigation) (summer blog posts)\n",
    "#how many heat complaints to date? (visualization)\n",
    "\n",
    "#A tale of two cities - adjust for pop (complaints per capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import csv\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = psycopg2.connect(database ='heatseek', user = 'heatseekroot', password = 'wearecoolsoweseekheat')\n",
    "    cursor = connection.cursor() #Open a cursor to perform operations\n",
    "    \n",
    "    cursor.execute('SELECT * from users') #Executes the query\n",
    "    users = cursor.fetchall() #cursor.fetchone() for one line, fetchmany() for multiple lines, fetchall() for all lines\n",
    "    users = pd.DataFrame(users) #Saves 'users' as a pandas dataframe\n",
    "    users_header = [desc[0] for desc in cursor.description] #This gets the descriptions from cursor.description \n",
    "    #(names are in the 0th index)\n",
    "    users.columns = users_header #PD array's column names\n",
    "    \n",
    "    cursor.execute('SELECT * FROM readings;')\n",
    "    readings = cursor.fetchall()\n",
    "    readings = pd.DataFrame(readings)\n",
    "    readings_header = [desc[0] for desc in cursor.description]\n",
    "    readings.columns = readings_header\n",
    "    \n",
    "    cursor.execute('SELECT * FROM sensors;')\n",
    "    sensors = cursor.fetchall()\n",
    "    sensors = pd.DataFrame(sensors)\n",
    "    sensors_header = [desc[0] for desc in cursor.description]\n",
    "    sensors.columns = sensors_header\n",
    "    \n",
    "    cursor.close() \n",
    "    connection.close()\n",
    "    \n",
    "except psycopg2.DatabaseError, error:\n",
    "    print 'Error %s' % error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This creates an array 'sensors_with_users' that consists of sensors that are currently assigned to users.\n",
    "sensors_with_users_raw = np.intersect1d(users.id.unique(), sensors.user_id.unique()) #Returns the common ids in both the datasets.\n",
    "#sensors.loc[sensors.user_id, sensors_with_users]\n",
    "sensors_with_users = []\n",
    "for ids in sensors_with_users_raw:\n",
    "    sensors_with_users.append(int(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function returns clean readings. #It doesn't exist yet\n",
    "#This function will return if a sensor is polling faster than once per hour (i.e., test cases)\n",
    "\n",
    "def dirty_data(dirty_readings, start_date = None, end_date = None):\n",
    "    if (start_date or end_date) == None:\n",
    "        start_date = pd.Timestamp('2000-01-01')\n",
    "        end_date = pd.Timestamp(datetime.datetime.now())\n",
    "    else:\n",
    "        start_date = pd.Timestamp(start_date)\n",
    "        end_date = pd.Timestamp(end_date)\n",
    "    \n",
    "    mask = (dirty_readings['created_at'] > start_date) & (dirty_readings['created_at'] <= end_date)\n",
    "    dirty_readings = dirty_readings.loc[mask]\n",
    "    \n",
    "    hot_ids = dirty_readings.loc[dirty_readings.temp > 90].sensor_id.unique() #Returns sensor IDs where indoor temp is > 90\n",
    "    cold_ids = dirty_readings.loc[dirty_readings.temp < 40].sensor_id.unique() #Returns sensor IDs where indoor temp is < 40\n",
    "    inside_colder_ids = dirty_readings.loc[dirty_readings.temp < dirty_readings.outdoor_temp].sensor_id.unique() #Returns sensor IDs where indoor temp is < outdoor temp\n",
    "    #Array of all the IDs above\n",
    "    all_ids = np.unique(np.concatenate((hot_ids, cold_ids, inside_colder_ids)))\n",
    "    all_ids = all_ids[~np.isnan(all_ids)]\n",
    "    #Create an empty dataframe with the IDs as indices\n",
    "    report = pd.DataFrame(index=all_ids,columns=['UserID','SensorID', 'Outside90', 'Inside40', 'InsideColderOutside'])\n",
    "    #Fill in the specific conditions as '1'\n",
    "    report.Outside90 = report.loc[hot_ids].Outside90.fillna(1)\n",
    "    report.Inside40 = report.loc[cold_ids].Inside40.fillna(1)\n",
    "    report.InsideColderOutside = report.loc[inside_colder_ids].InsideColderOutside.fillna(1)\n",
    "    report = report.fillna(0)\n",
    "    report.SensorID = report.index\n",
    "    \n",
    "    #Fill in UserIDs\n",
    "    problem_ids = sensors[sensors.id.isin(all_ids)]\n",
    "    for index in report.index.values:\n",
    "        index = int(index)\n",
    "        try:\n",
    "            report.loc[index, 'UserID'] = sensors.loc[index, 'user_id']\n",
    "        except KeyError:\n",
    "            report.loc[index, 'UserID']  = 'No such user in sensors table.'\n",
    "    return report\n",
    "        \n",
    "\n",
    "\n",
    "def clean_data(dirty_readings):\n",
    "    cleaner_readings = dirty_readings[dirty_readings.sensor_id.notnull()] #Remove cases where there are no sensor IDs\n",
    "    return cleaner_readings\n",
    "    \n",
    "clean_readings = clean_data(readings)\n",
    "report = dirty_data(readings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function takes (start date, end date, sensor id), returns % of failure\n",
    "def sensor_down_complete(data, start_date, end_date, sensor_id): \n",
    "    \n",
    "    #This pulls up the tenant's first and last name.\n",
    "    try:\n",
    "        tenant_id = int(sensors.loc[sensors.id == sensor_id].user_id.values[0])\n",
    "        tenant_first_name = users.loc[users.id == tenant_id].first_name.values[-1] #This pulls up the first name on the list (not the most recent)\n",
    "        tenant_last_name = users.loc[users.id == tenant_id].last_name.values[-1]\n",
    "    #Are these really not assigned?\n",
    "    except ValueError:\n",
    "        tenant_id = 'None'\n",
    "        tenant_first_name = 'Not'\n",
    "        tenant_last_name = 'Assigned'\n",
    "    except IndexError:\n",
    "        tenant_id = 'None'\n",
    "        tenant_first_name = 'Not'\n",
    "        tenant_last_name = 'Assigned'\n",
    "        \n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "\n",
    "    sensor_readings = data.loc[data.sensor_id == sensor_id]\n",
    "    \n",
    "    #Converting to timestamps\n",
    "    #for i in sensor_readings.index.values: #Iterates through all the index values\n",
    "        #sensor_readings.loc[i, 'created_at'] = pd.Timestamp(sensor_readings.created_at[i])\n",
    "    #Using map instead of for loop (about 15-20x faster)\n",
    "    try:\n",
    "        sensor_readings.loc[:, 'created_at'] = map(pd.Timestamp, sensor_readings.created_at)\n",
    "    except TypeError:\n",
    "        tenant_first_name = 'Mapping Error'\n",
    "        tenant_last_name = 'Only One Entry'\n",
    "        pass\n",
    "    #Using list comprehensions (as efficient as map)\n",
    "    #sensor_readings.loc[:, 'created_at'] = [pd.Timestamp(x) for x in sensor_readings.created_at]\n",
    "        \n",
    "    #Using a boolean mask to select readings between the two dates \n",
    "    #(http://stackoverflow.com/questions/29370057/select-dataframe-rows-between-two-dates)   \n",
    "    mask = (sensor_readings['created_at'] > start_date) & (sensor_readings['created_at'] <= end_date)\n",
    "    masked_sensor_readings = sensor_readings.loc[mask] #Get all readings between the two dates\n",
    "    masked_sensor_readings = masked_sensor_readings.sort_values('created_at')\n",
    "    #We then calculate how many hours have passed for that specific sensor and date range\n",
    "    try:\n",
    "        sensor_readings_start_date = masked_sensor_readings.loc[masked_sensor_readings.index.values[0], 'created_at']\n",
    "        sensor_readings_end_date = \\\n",
    "        masked_sensor_readings.loc[masked_sensor_readings.index.values[len(masked_sensor_readings)-1], 'created_at']\n",
    "        timedelta_in_seconds =  sensor_readings_end_date - sensor_readings_start_date #This returns Timedelta object\n",
    "        timedelta_in_seconds = timedelta_in_seconds.total_seconds()\n",
    "        total_number_of_hours = timedelta_in_seconds/3600 + 1 #The +1 fixes the rounding error for now but IDK why yet.\n",
    "        \n",
    "        hours_in_date_range = ((end_date-start_date).total_seconds())/3600 + 1\n",
    "        \n",
    "    except IndexError:\n",
    "        return [tenant_first_name, tenant_last_name, sensor_id, tenant_id, \"No valid readings during this time frame.\"]\n",
    "    \n",
    "    proportion_of_total_uptime = (len(masked_sensor_readings)/hours_in_date_range) * 100 #Proportion of uptime over TOTAL HOURS\n",
    "    proportion_within_sensor_uptime = (len(masked_sensor_readings)/total_number_of_hours) * 100 #Proportion of uptime for the sensor's first and last uploaded dates.\n",
    "    if proportion_within_sensor_uptime <= 100.1:\n",
    "        return [tenant_first_name, tenant_last_name, sensor_id, tenant_id, proportion_of_total_uptime, proportion_within_sensor_uptime]\n",
    "    else:\n",
    "        return [tenant_first_name, tenant_last_name, sensor_id, tenant_id, proportion_of_total_uptime, proportion_within_sensor_uptime, 'Sensor has readings more frequent than once per hour. Check readings table.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function takes (start date, end date, sensor id), returns % of failure\n",
    "def sensor_down(data, start_date, end_date, sensor_id): \n",
    "    \n",
    "    #This pulls up the tenant's first and last name.\n",
    "    try:\n",
    "        tenant_id = int(sensors.loc[sensors.id == sensor_id].user_id.values[0])\n",
    "        tenant_first_name = users.loc[users.id == tenant_id].first_name.values[-1] #This pulls up the first name on the list (not the most recent)\n",
    "        tenant_last_name = users.loc[users.id == tenant_id].last_name.values[-1]\n",
    "    #Are these really not assigned?\n",
    "    except ValueError:\n",
    "        tenant_id = 'None'\n",
    "        tenant_first_name = 'Not'\n",
    "        tenant_last_name = 'Assigned'\n",
    "    except IndexError:\n",
    "        tenant_id = 'None'\n",
    "        tenant_first_name = 'Not'\n",
    "        tenant_last_name = 'Assigned'\n",
    "        \n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "\n",
    "    sensor_readings = data.loc[data.sensor_id == sensor_id]\n",
    "    \n",
    "    #Converting to timestamps\n",
    "    #for i in sensor_readings.index.values: #Iterates through all the index values\n",
    "        #sensor_readings.loc[i, 'created_at'] = pd.Timestamp(sensor_readings.created_at[i])\n",
    "    #Using map instead of for loop (about 15-20x faster)\n",
    "    try:\n",
    "        sensor_readings.loc[:, 'created_at'] = map(pd.Timestamp, sensor_readings.created_at)\n",
    "    except TypeError:\n",
    "        tenant_first_name = 'Mapping Error'\n",
    "        tenant_last_name = 'Only One Entry'\n",
    "        pass\n",
    "    #Using list comprehensions (as efficient as map)\n",
    "    #sensor_readings.loc[:, 'created_at'] = [pd.Timestamp(x) for x in sensor_readings.created_at]\n",
    "        \n",
    "    #Using a boolean mask to select readings between the two dates \n",
    "    #(http://stackoverflow.com/questions/29370057/select-dataframe-rows-between-two-dates)   \n",
    "    mask = (sensor_readings['created_at'] > start_date) & (sensor_readings['created_at'] <= end_date)\n",
    "    masked_sensor_readings = sensor_readings.loc[mask] #Get all readings between the two dates\n",
    "    masked_sensor_readings = masked_sensor_readings.sort_values('created_at')\n",
    "    #We then calculate how many hours have passed for that specific sensor and date range\n",
    "    try:\n",
    "        sensor_readings_start_date = masked_sensor_readings.loc[masked_sensor_readings.index.values[0], 'created_at']\n",
    "        sensor_readings_end_date = \\\n",
    "        masked_sensor_readings.loc[masked_sensor_readings.index.values[len(masked_sensor_readings)-1], 'created_at']\n",
    "        timedelta_in_seconds =  sensor_readings_end_date - sensor_readings_start_date #This returns Timedelta object\n",
    "        timedelta_in_seconds = timedelta_in_seconds.total_seconds()\n",
    "        total_number_of_hours = timedelta_in_seconds/3600 + 1 #The +1 fixes the rounding error for now but IDK why yet.\n",
    "        \n",
    "        hours_in_date_range = ((end_date-start_date).total_seconds())/3600 + 1\n",
    "        proportion_of_total_uptime = (len(masked_sensor_readings)/hours_in_date_range) * 100 #Proportion of uptime over TOTAL HOURS\n",
    "        proportion_within_sensor_uptime = (len(masked_sensor_readings)/total_number_of_hours) * 100 #Proportion of uptime for the sensor's first and last uploaded dates.\n",
    "        \n",
    "        if proportion_within_sensor_uptime <= 100.1:\n",
    "            return [sensor_id, 255*(proportion_within_sensor_uptime/100), proportion_of_total_uptime, tenant_first_name, tenant_last_name, tenant_id]\n",
    "        else:\n",
    "            pass\n",
    "    except IndexError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert input to Timestamp",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-e0469f3a0660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mviolation_percentages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2016-01-01'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2016-02-07'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m19\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-e0469f3a0660>\u001b[0m in \u001b[0;36mviolation_percentages\u001b[1;34m(data, start_date, end_date, sensor_id)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m#masks for day and night session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mday_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msensor_readings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mday_start\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msensor_readings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mnight_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0mnight_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msensor_readings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnight_start\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msensor_readings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mday_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dfthd\\Anaconda3\\envs\\python2\\lib\\site-packages\\pandas\\core\\ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other, axis)\u001b[0m\n\u001b[0;32m    724\u001b[0m                 \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m                 raise TypeError('Could not compare %s type with Series'\n",
      "\u001b[1;32mC:\\Users\\dfthd\\Anaconda3\\envs\\python2\\lib\\site-packages\\pandas\\core\\ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.convert_scalar (pandas\\index.c:12582)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.convert_scalar (pandas\\index.c:12189)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\tslib.pyx\u001b[0m in \u001b[0;36mpandas.tslib.Timestamp.__new__ (pandas\\tslib.c:8967)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\tslib.pyx\u001b[0m in \u001b[0;36mpandas.tslib.convert_to_tsobject (pandas\\tslib.c:23508)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert input to Timestamp"
     ]
    }
   ],
   "source": [
    "#from the time a sensor has been deployed till now, how many % of the total hours where it is possible to receive a violation are actually in violation.\n",
    "#minimum of 30 days\n",
    "\n",
    "#account for sensor downtime\n",
    "#instead of calculating proportions over 100%, we can calculate them over the % of time the sensor was actually - so basically, if it wasn't up\n",
    "#we assume that there was no violation\n",
    "\n",
    "def violation_percentages(data, start_date, end_date, sensor_id):\n",
    "    \n",
    "    sensor_readings = data.loc[data.sensor_id == sensor_id] #All readings for sensorIDs\n",
    "    try:\n",
    "        sensor_readings.loc[:,'created_at'] = map(pd.Timestamp, sensor_readings.created_at) #convert all to timestampst\n",
    "    except TypeError:\n",
    "        pass\n",
    "    \n",
    "    #Filter out sensors that are < 30 days old\n",
    "    sensor_readings_start_date = sensor_readings.loc[sensor_readings.index.values[0], 'created_at'].date()\n",
    "    today = date.today()\n",
    "    datediff = today - sensor_readings_start_date\n",
    "    \n",
    "    if datediff.days < 30: #If a sensor has been up for < 30 days, don't do anything\n",
    "        pass\n",
    "    else:\n",
    "        start_date = pd.Timestamp(start_date) #Convert dates to pd.Timestamp\n",
    "        end_date = pd.Timestamp(end_date)\n",
    "    \n",
    "        mask = (sensor_readings['created_at'] > start_date) & (sensor_readings['created_at'] <= end_date) #mask for date range\n",
    "        masked_sensor_readings = sensor_readings.loc[mask]\n",
    "    \n",
    "    \n",
    "    #day_data\n",
    "    #night_data\n",
    "    \n",
    "    #calc total hours for night data\n",
    "    #calc total hours for day data\n",
    "    #count violations for night data\n",
    "    #count violations for day data\n",
    "    \n",
    "    #masked_sensor_readings = sensor_readings.loc[mask] \n",
    "        try:\n",
    "            \n",
    "            sensor_readings_start_date = masked_sensor_readings.loc[masked_sensor_readings.index.values[0], 'created_at']\n",
    "            sensor_readings_end_date = \\\n",
    "            masked_sensor_readings.loc[masked_sensor_readings.index.values[len(masked_sensor_readings)-1], 'created_at']\n",
    "            timedelta_in_seconds =  sensor_readings_end_date - sensor_readings_start_date #This returns Timedelta object\n",
    "            timedelta_in_seconds = timedelta_in_seconds.total_seconds()\n",
    "            total_number_of_hours = timedelta_in_seconds/3600 + 1 #The +1 fixes the rounding error for now but IDK why yet.\n",
    "            #how many ours in this date range in total\n",
    "            #day_start = datetime.time(6)\n",
    "            #night_start = datetime.time(22)\n",
    "            \n",
    "            #masks for day and night session\n",
    "            #day_mask = (sensor_readings['created_at'] > day_start) & (sensor_readings['created_at'] <= night_start)\n",
    "            #night_mask = (sensor_readings['created_at'] > night_start) & (sensor_readings['created_at'] <= day_start)\n",
    "            \n",
    "            day_readings = sensor_readings.indexer_between_time\n",
    "            \n",
    "            hours_in_date_range = ((end_date-start_date).total_seconds())/3600 + 1\n",
    "            proportion_of_total_uptime = (len(masked_sensor_readings)/hours_in_date_range) * 100 #Proportion of uptime over TOTAL HOURS\n",
    "            proportion_within_sensor_uptime = (len(masked_sensor_readings)/total_number_of_hours) * 100 #Proportion of uptime for the sensor's first and last uploaded dates.\n",
    "    \n",
    "            return masked_sensor_readings\n",
    "        except IndexError:\n",
    "            pass\n",
    "    \n",
    "violation_percentages(readings, '2016-01-01', '2016-02-07', 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 99.99980709913753, 99.31034482758618, 'Loraine', 'Dellamore', 245]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfthd\\Anaconda3\\envs\\python2\\lib\\site-packages\\pandas\\core\\indexing.py:420: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "print sensor_down(readings, '2016-02-01', '2016-02-07', 19)\n",
    "#mask = (readings['created_at'] > pd.Timestamp('2016-02-01')) & (readings['created_at'] <= pd.Timestamp('2016-02-07'))\n",
    "#readings.loc[readings.sensor_id == 296]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function creates a simulated dataset of readings.\n",
    "def simulate_data(start_date, end_date, polling_rate, sensor_id): #polling_rate in minutes\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "\n",
    "    #how many hours between the two dates:\n",
    "    timedelta_in_seconds = end_date-start_date\n",
    "    total_number_of_hours = timedelta_in_seconds.total_seconds()/(polling_rate*60)\n",
    "    \n",
    "    #Create an empty pandas dataframe\n",
    "    index = xrange(1,int(total_number_of_hours)+1)\n",
    "    columns = ['created_at', 'sensor_id']\n",
    "    simulated_readings = pd.DataFrame(index = index, columns = columns)\n",
    "    simulated_readings.loc[:,'sensor_id'] = sensor_id\n",
    "    \n",
    "    #Populate it with columns of 'create_at' dates\n",
    "    time_counter = start_date\n",
    "    for i in simulated_readings.index.values:\n",
    "        simulated_readings.loc[i,'created_at'] = time_counter\n",
    "        time_counter = time_counter + pd.Timedelta('00:%s:00' % polling_rate)\n",
    "   \n",
    "    return simulated_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing stuff\n",
    "#simulated_data = simulate_data('2015-01-01', '2015-01-31', 60, 30)\n",
    "#print sensor_down(readings, '2015-01-01', '2016-01-01', 26) #I think this works\n",
    "#print sensor_down(simulated_data, '2015-01-01', '2015-01-31', 30) #There's some rounding error\n",
    "#for user_id in sensors_with_users:\n",
    "    #latest_id = sensors.loc[sensors.user_id==ids].id\n",
    "    #latest_id = latest_id.index.values[len(latest_id)-1]\n",
    "    #print sensors.loc[sensors.user_id == ids].id\n",
    "    #try:\n",
    "    #print sensor_down(readings, '2015-01-01', '2016-01-01', sensors.loc[sensors.user_id == user_id].id.values[0])\n",
    "    #print users.loc[users.id == user_id].first_name\n",
    "    #except IndexError:\n",
    "    #print \"Error.\"\n",
    "    #print sensors.loc[sensors.user_id == user_id].id.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function generates a report; we might want to make this a cron job.\n",
    "def generate_report(start_date, end_date):\n",
    "    report = []\n",
    "    sensor_ids = readings.sensor_id.unique()\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "    for ids in sensor_ids:\n",
    "        temp = sensor_down(readings, start_date, end_date, ids)\n",
    "        if temp != None:\n",
    "            report.append(temp)\n",
    "        else:\n",
    "            pass\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7389025019316477"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "report = generate_report('2016-02-01','2016-02-07')\n",
    "header =['sensorID', 'status', 'Percentage of uptime in daterange', 'FirstName', 'LastName' , 'userID']\n",
    "\n",
    "toc = time.clock()\n",
    "toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserID\n",
      "SensorID\n",
      "Outside90\n",
      "Inside40\n",
      "InsideColderOutside\n"
     ]
    }
   ],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "report = dirty_data(readings)\n",
    "header = ['UserID', 'SensorID', 'Outside90', 'Inside40', 'InsideColderOutside']\n",
    "\n",
    "toc = time.clock()\n",
    "toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvoutput = open('sensors.csv', 'wb')\n",
    "writer = csv.writer(csvoutput)\n",
    "writer.writerow(header)\n",
    "for i in report:\n",
    "    writer.writerow(i)\n",
    "csvoutput.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report.to_csv('dirtydata.csv', index = False, na_rep=\"Not Currently Assigned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sensordata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-0bc7b148d9be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#We first convert the string dates into datetime format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0msensordata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'formatteddate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msensordata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%Y-%m-%d %H:%M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#Then, one way of telling if a user_id was an actual user or a test case was to calculate the average timedelta for each user_id.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sensordata' is not defined"
     ]
    }
   ],
   "source": [
    "#What criteria do we want to use.\n",
    "\n",
    "#SENSOR NUMBERS AS IDENTIFIERS, SOME DEGREE OF SEVERITY OF PROBLEM (CATEGORICAL)\n",
    "\n",
    "#for the criteria we go, how do we define \"bad\"\n",
    "#total days\n",
    "#proportion of days (or clusters?)\n",
    "#temperature discrepancy\n",
    "#multiple apartments in the same building (how many % of the apartments in a building and how bad)\n",
    "#multiple buildings by the same landlord (how many % of the buildings a landlord owns are bad)\n",
    "#are our sensors failing in a specific building?\n",
    "\n",
    "#Getting rid of test cases:\n",
    "#1. Can we just delete by test IDs?\n",
    "#2. If testing was separated by a minute, we can find all test cases by looping through all users,\n",
    "#  and if they have a bunch of data that was collected within minutes, delete the user?\n",
    "\n",
    "#We first convert the string dates into datetime format\n",
    "sensordata['formatteddate'] = sensordata.created_at.apply(lambda x: pd.to_datetime(x,  format = \"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "#Then, one way of telling if a user_id was an actual user or a test case was to calculate the average timedelta for each user_id.\n",
    "#Timedeltas of 1min are tests, 1 hour are users (don't know if this is always true, but if no user has an average polling rate\n",
    "#of 1 min, we can use a bunch of methods to filter away test cases step by step).\n",
    "\n",
    "sensordata['averagetimedelta'] = 0.00 #makes a new column\n",
    "\n",
    "for i in sensordata.user_id.unique(): #for each user\n",
    "    timelist = sensordata.loc[sensordata.user_id == i, 'formatteddate'] #this gives us a list of all their times in timestamp\n",
    "    timedeltas = [] \n",
    "    for j in range(1, len(timelist)-1):\n",
    "        timedeltas.append(timelist.iloc[j] - timelist.iloc[j-1]) #list of differences in time between time point j and j-1\n",
    "    try:\n",
    "        #we print the user_id, followd by their average time delta from point j to j-1\n",
    "        print i, abs(sum(timedeltas, datetime.timedelta(0)))/len(timedeltas) #the average timedelta\n",
    "    except ZeroDivisionError: #some cases have too few points (and results in a zero division error)\n",
    "        #instead of breaking with we encounter a zerodivisionerror, just print the following:\n",
    "        print i, \"Too few data points?\"\n",
    "    sensordata.loc[sensordata.user_id == i, 'averagetimedelta'] = averagetimedeltas.total_seconds()\n",
    "\n",
    "#3. If user ids are recycled, we'll have to do a combination of those things.\n",
    "\n",
    "#Some sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Triage\n",
    "#We want to have a measure of which users are facing the most chronic problems.\n",
    "\n",
    "#Metric combining temperature difference and chronicity of problems\n",
    "\n",
    "#Write some code that subsets all the violation == 't' cases\n",
    "sensordataviolations = sensordata[sensordata.violation == 't'] #here it is.\n",
    "\n",
    "#Hackiest method: just number of violations/numberof nonviolations and sort users by that\n",
    "#That is, which users have had the most violations given the total number of readings \n",
    "\n",
    "violationsovertime = []\n",
    "\n",
    "for i in sensordata.user_id.unique():\n",
    "    nonviolations = sensordata.loc[sensordata.user_id == i, 'violation'].value_counts()['f'] #Number of violations = 'f'\n",
    "    try:\n",
    "        violations = sensordata.loc[sensordata.user_id == i, 'violation'].value_counts()['t'] #Number of violations = 't'\n",
    "    except KeyError:\n",
    "        violations = 0    \n",
    "    sensordata.loc[sensordata.user_id == i, 'vfreq'] = float(violations)/float(nonviolations)\n",
    "    violationsovertime.append([i, (float(violations)/float(nonviolations))])\n",
    "\n",
    "#violations over time gives first the user_id, then the proportion of how many of their readings are violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stuff to do for fun\n",
    "\n",
    "#Variability/consistency\n",
    "#Which buildings have the least/most variable temperatures?\n",
    "#For this, we just calculate within-person variability (how much do sensor temperatures by the same user) vary as a function of time\n",
    "#We an use this same process to calculate variability between locations (e.g., just calculate variance for each location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now, we loop over all unique users in the dataset and generate a measure of how long they've had the sensor running\n",
    "sensordata['totaltime'] = 0\n",
    "sensordata['vfreq'] = 0\n",
    "\n",
    "for i in sensordata.user_id.unique():\n",
    "    firstentry = len(sensordata.loc[sensordata.user_id==i,'formatteddate']) #This gives us the index of the first timepoint\n",
    "    lasttime = sensordata.loc[sensordata.user_id == i, 'formatteddate'].iloc[0] #This was the timestamp of the latest timepoint\n",
    "    firsttime =  sensordata.loc[sensordata.user_id == i, 'formatteddate'].iloc[firstentry-1] #This was the timestamp of the first timepoint\n",
    "    sensordata.loc[sensordata.user_id == i, 'totaltime'] = lasttime - firsttime #This is the timedelta (over how long a period readings were made)\n",
    "    #print i, lasttime-firsttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " [19.0, 99.99980709913753, 99.31034482758618, 'Loraine', 'Dellamore', 245],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [98.0, 96.36217633066165, 36.55172413793103, 'Christopher', 'Lamar', 266],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [184.0, 95.8331484700068, 95.17241379310343, 'Tammy', 'Brake', 254],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [154.0, 99.99942129964523, 99.31034482758618, 'Annette ', 'Cummings ', 251],\n",
       " None,\n",
       " [145.0, 100.00038580395756, 99.31034482758618, 'Jane', 'Angel', 199],\n",
       " [167.0, 97.91534450730795, 97.24137931034481, 'Dara', 'Soukamneuth', 200],\n",
       " [181.0, 99.99942129964523, 99.31034482758618, 'Redoneva', 'Andrews', 202],\n",
       " None,\n",
       " [178.0, 99.99845681393805, 99.31034482758618, 'Helen', 'Pugmire', 201],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [159.0, 99.9976852387676, 99.31034482758618, 'Miguel', 'Rios', 208],\n",
       " [90.0, 99.3645916169329, 98.6206896551724, 'Virgen', 'Martinez-Lopez', 212],\n",
       " [88.0, 92.91041887113839, 89.65517241379308, 'Nancy', 'De Jes\\xc3\\xbas', 211],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [188.0, 99.99980709913753, 99.31034482758618, 'Malaika', 'Quashie', 135],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [189.0, 99.99903550312979, 99.31034482758618, 'Not', 'Assigned', 'None'],\n",
       " [193.0, 99.99884260598833, 99.31034482758618, 'Susana', 'Robalino', 170],\n",
       " [194.0, 99.30478931489725, 98.6206896551724, 'Indira', 'Rodney', 225],\n",
       " [195.0, 99.9986497095911, 99.31034482758618, 'Janelle', 'Gedeon', 226],\n",
       " [191.0, 99.30574711756773, 98.6206896551724, 'Constance', 'Taylor', 223],\n",
       " [192.0, 99.99961419901928, 99.31034482758618, 'Not', 'Assigned', 'None'],\n",
       " [296.0,\n",
       "  780.276347873205,\n",
       "  5.517241379310344,\n",
       "  'Blue Ridge',\n",
       "  'Labs',\n",
       "  169,\n",
       "  'Sensor has readings more frequent than once per hour. Check readings table.'],\n",
       " [155.0, 98.61187200518522, 97.9310344827586, 'Kenny', 'Harewood', 195],\n",
       " [267.0, 99.99942129964523, 99.31034482758618, 'Hazel-Ann', 'FTC', 264],\n",
       " [276.0, 99.99822698179108, 97.24137931034481, 'Aurelia', 'Cruz', 240],\n",
       " [283.0, 83.59817603979549, 83.44827586206895, 'Iris', 'Brown ', 241],\n",
       " None,\n",
       " None,\n",
       " [285.0, 99.99999999999997, 99.31034482758618, 'Ramona', 'Garcia ', 260],\n",
       " [271.0, 99.99845681393805, 99.31034482758618, 'Carolyn', 'Butler ', 250],\n",
       " [292.0, 99.99922840101542, 99.31034482758618, 'Roylyn ', 'Henery', 238],\n",
       " [11.0, 99.9975490796794, 23.44827586206896, 'Desiree', 'Browne', 239],\n",
       " [160.0, 96.54845076781486, 95.86206896551722, 'Arlene', 'Riley', 242],\n",
       " [274.0, 99.99980709913753, 99.31034482758618, 'Denise', 'Johnson ', 258],\n",
       " [228.0, 99.99961419901928, 99.31034482758618, 'Lisa', 'Mathis', 194],\n",
       " None,\n",
       " [268.0, 98.6113013335288, 97.9310344827586, 'Wanda', 'Perez', 261],\n",
       " [214.0, 90.37037037037035, 84.13793103448273, 'Annery', 'Ortiz', 203],\n",
       " [282.0, 42.85835766319907, 14.482758620689653, 'Julio', 'Belmonte ', 262],\n",
       " [27.0, 99.99961419901928, 99.31034482758618, 'Charles', 'Cordrey', 236],\n",
       " [190.0, 99.99421329784155, 99.31034482758618, 'Rita', 'Kettrles', 246],\n",
       " [85.0, 94.44395862161204, 35.17241379310344, 'Korima', 'Rock', 267],\n",
       " [288.0, 99.99945534065719, 35.17241379310344, 'Nioka', 'Jenkins', 268],\n",
       " [273.0, 76.59438662435063, 24.827586206896544, 'Tanya', 'Thomas', 213],\n",
       " None,\n",
       " [201.0, 99.30651337300705, 98.6206896551724, 'Odalis', 'Polanco', 222],\n",
       " [277.0, 99.30498087395326, 98.6206896551724, 'Yolanda', 'Rosado', 270],\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sensors.user_id.value_counts()\n",
    "#sensordata.violation.value_counts() #This returns the number of 't's and 'f's\n",
    "#np.sort(userdata.id.unique())\n",
    "#np.intersect1d(userdata.id.unique(), sensordata.user_id.unique()) #Returns the common ids in both the datasets.\n",
    "#readings.sensor_id.unique\n",
    "\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
