{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To deploy:\n",
    "\n",
    "1. Set up a connection to the database\n",
    "2. Run dirty_data method with the last 24 hours or whatever as arguments. This produces a report of IDs with bad data that can be put on a static webpage.\n",
    "3. Run clean_data method - not sure what this does yet - and update the postgres \\_cleaned table\n",
    "4. Run generate_report method with the last 24 hours or whatever as arguments. This produces a report of the sensors that are down that can be put on a static webpage. \n",
    "\n",
    "The above methods all produce pandas dataframes that can be written to_csv() or to_sql().\n",
    "\n",
    "The methods violation_percentages and violation_report can be run when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import csv\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = psycopg2.connect(database ='heatseek', user = 'heatseekroot', password = 'wearecoolsoweseekheat')\n",
    "    cursor = connection.cursor() #Open a cursor to perform operations\n",
    "    \n",
    "    cursor.execute('SELECT * from users') #Executes the query\n",
    "    users = cursor.fetchall() #cursor.fetchone() for one line, fetchmany() for multiple lines, fetchall() for all lines\n",
    "    users = pd.DataFrame(users) #Saves 'users' as a pandas dataframe\n",
    "    users_header = [desc[0] for desc in cursor.description] #This gets the descriptions from cursor.description \n",
    "    #(names are in the 0th index)\n",
    "    users.columns = users_header #PD array's column names\n",
    "    \n",
    "    cursor.execute('SELECT * FROM readings;')\n",
    "    readings = cursor.fetchall()\n",
    "    readings = pd.DataFrame(readings)\n",
    "    readings_header = [desc[0] for desc in cursor.description]\n",
    "    readings.columns = readings_header\n",
    "    \n",
    "    cursor.execute('SELECT * FROM sensors;')\n",
    "    sensors = cursor.fetchall()\n",
    "    sensors = pd.DataFrame(sensors)\n",
    "    sensors_header = [desc[0] for desc in cursor.description]\n",
    "    sensors.columns = sensors_header\n",
    "    \n",
    "    cursor.close() \n",
    "    connection.close()\n",
    "    \n",
    "except psycopg2.DatabaseError, error:\n",
    "    print 'Error %s' % error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This creates an array 'sensors_with_users' that consists of sensors that are currently assigned to users.\n",
    "sensors_with_users = sorted([x for x in users.id.unique() if x in  sensors.user_id.unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function returns clean readings. #It doesn't exist yet\n",
    "#This function will return if a sensor is polling faster than once per hour (i.e., test cases)\n",
    "\n",
    "def dirty_data(dirty_readings, start_date = None, end_date = None):\n",
    "    if (start_date or end_date) == None:\n",
    "        start_date = pd.Timestamp('2000-01-01')\n",
    "        end_date = pd.Timestamp(datetime.datetime.now())\n",
    "    else:\n",
    "        start_date = pd.Timestamp(start_date)\n",
    "        end_date = pd.Timestamp(end_date)\n",
    "    \n",
    "    mask = (dirty_readings['created_at'] > start_date) & (dirty_readings['created_at'] <= end_date)\n",
    "    dirty_readings = dirty_readings.loc[mask]\n",
    "    \n",
    "    hot_ids = dirty_readings.loc[dirty_readings.temp > 90].sensor_id.unique() #Returns sensor IDs where indoor temp is > 90\n",
    "    cold_ids = dirty_readings.loc[dirty_readings.temp < 40].sensor_id.unique() #Returns sensor IDs where indoor temp is < 40\n",
    "    inside_colder_ids = dirty_readings.loc[dirty_readings.temp < dirty_readings.outdoor_temp].sensor_id.unique() #Returns sensor IDs where indoor temp is < outdoor temp\n",
    "    #Array of all the IDs above\n",
    "    all_ids = np.unique(np.concatenate((hot_ids, cold_ids, inside_colder_ids)))\n",
    "    all_ids = all_ids[~np.isnan(all_ids)]\n",
    "    #Create an empty dataframe with the IDs as indices\n",
    "    report = pd.DataFrame(index=all_ids,columns=['UserID','SensorID', 'Outside90', 'Inside40', 'InsideColderOutside'])\n",
    "    #Fill in the specific conditions as '1'\n",
    "    report.Outside90 = report.loc[hot_ids].Outside90.fillna(1)\n",
    "    report.Inside40 = report.loc[cold_ids].Inside40.fillna(1)\n",
    "    report.InsideColderOutside = report.loc[inside_colder_ids].InsideColderOutside.fillna(1)\n",
    "    report = report.fillna(0)\n",
    "    report.SensorID = report.index\n",
    "    \n",
    "    #Fill in UserIDs\n",
    "    problem_ids = sensors[sensors.id.isin(all_ids)]\n",
    "    for index in report.index.values:\n",
    "        index = int(index)\n",
    "        try:\n",
    "            report.loc[index, 'UserID'] = sensors.loc[index, 'user_id']\n",
    "        except KeyError:\n",
    "            report.loc[index, 'UserID']  = 'No such user in sensors table.'\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(dirty_readings):\n",
    "    cleaner_readings = dirty_readings[dirty_readings.sensor_id.notnull()] #Remove cases where there are no sensor IDs\n",
    "    return cleaner_readings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function takes (start date, end date, sensor id), returns % of failure\n",
    "def sensor_down(data, start_date, end_date, sensor_id): \n",
    "    \n",
    "    #This pulls up the tennant's first and last name.\n",
    "    try:\n",
    "        tennant_id = int(sensors.loc[sensors.id == sensor_id].user_id.values[0])\n",
    "        tennant_first_name = users.loc[users.id == tennant_id].first_name.values[-1] #This pulls up the first name on the list (not the most recent)\n",
    "        tennant_last_name = users.loc[users.id == tennant_id].last_name.values[-1]\n",
    "    #Are these really not assigned?\n",
    "    except ValueError:\n",
    "        tennant_id = 'None'\n",
    "        tennant_first_name = 'Not'\n",
    "        tennant_last_name = 'Assigned'\n",
    "    except IndexError:\n",
    "        tennant_id = 'None'\n",
    "        tennant_first_name = 'Not'\n",
    "        tennant_last_name = 'Assigned'\n",
    "        \n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "\n",
    "    sensor_readings = data.loc[data.sensor_id == sensor_id]\n",
    "    \n",
    "    #Converting to timestamps\n",
    "    #for i in sensor_readings.index.values: #Iterates through all the index values\n",
    "        #sensor_readings.loc[i, 'created_at'] = pd.Timestamp(sensor_readings.created_at[i])\n",
    "    #Using map instead of for loop (about 15-20x faster)\n",
    "    try:\n",
    "        sensor_readings.loc[:, 'created_at'] = map(pd.Timestamp, sensor_readings.created_at)\n",
    "    except TypeError:\n",
    "        tennant_first_name = 'Mapping Error'\n",
    "        tennant_last_name = 'Only One Entry'\n",
    "        pass\n",
    "    #Using list comprehensions (as efficient as map)\n",
    "    #sensor_readings.loc[:, 'created_at'] = [pd.Timestamp(x) for x in sensor_readings.created_at]\n",
    "        \n",
    "    #Using a boolean mask to select readings between the two dates \n",
    "    #(http://stackoverflow.com/questions/29370057/select-dataframe-rows-between-two-dates)   \n",
    "    mask = (sensor_readings['created_at'] > start_date) & (sensor_readings['created_at'] <= end_date)\n",
    "    masked_sensor_readings = sensor_readings.loc[mask] #Get all readings between the two dates\n",
    "    masked_sensor_readings = masked_sensor_readings.sort_values('created_at')\n",
    "    #We then calculate how many hours have passed for that specific sensor and date range\n",
    "    try:\n",
    "        sensor_readings_start_date = masked_sensor_readings.loc[masked_sensor_readings.index.values[0], 'created_at']\n",
    "        sensor_readings_end_date = \\\n",
    "        masked_sensor_readings.loc[masked_sensor_readings.index.values[len(masked_sensor_readings)-1], 'created_at']\n",
    "        timedelta_in_seconds =  sensor_readings_end_date - sensor_readings_start_date #This returns Timedelta object\n",
    "        timedelta_in_seconds = timedelta_in_seconds.total_seconds()\n",
    "        total_number_of_hours = timedelta_in_seconds/3600 + 1 #The +1 fixes the rounding error for now but IDK why yet.\n",
    "        \n",
    "        hours_in_date_range = ((end_date-start_date).total_seconds())/3600 + 1\n",
    "        \n",
    "    except IndexError:\n",
    "        return [tennant_first_name, tennant_last_name, sensor_id, tennant_id, \"No valid readings during this time frame.\"]\n",
    "    \n",
    "    proportion_of_total_uptime = (len(masked_sensor_readings)/hours_in_date_range) * 100 #Proportion of uptime over TOTAL HOURS\n",
    "    proportion_within_sensor_uptime = (len(masked_sensor_readings)/total_number_of_hours) * 100 #Proportion of uptime for the sensor's first and last uploaded dates.\n",
    "    if proportion_within_sensor_uptime <= 100.1:\n",
    "        return [tennant_first_name, tennant_last_name, sensor_id, tennant_id, proportion_of_total_uptime, proportion_within_sensor_uptime]\n",
    "    else:\n",
    "        return [tennant_first_name, tennant_last_name, sensor_id, tennant_id, proportion_of_total_uptime, proportion_within_sensor_uptime, 'Sensor has readings more frequent than once per hour. Check readings table.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def violation_percentages(data, start_date, end_date, sensor_id):\n",
    "    \n",
    "    sensor_readings = data.loc[data.sensor_id == sensor_id] #All readings for a sensorID\n",
    "    try:\n",
    "        sensor_readings.loc[:,'created_at'] = map(pd.Timestamp, sensor_readings.created_at) #convert all to timestampst\n",
    "    except TypeError:\n",
    "        pass\n",
    "    \n",
    "    #Filter out sensors that are < 30 days old\n",
    "    try:\n",
    "        sensor_readings_start_date = sensor_readings.loc[sensor_readings.index.values[0], 'created_at'].date()\n",
    "        today = date.today()\n",
    "        datediff = today - sensor_readings_start_date\n",
    "    except:\n",
    "        return \"No readings in date range.\"\n",
    "    \n",
    "    if datediff.days < 30: #If a sensor has been up for < 30 days, don't do anything\n",
    "        pass\n",
    "    else:\n",
    "        start_date = pd.Timestamp(start_date) #Convert dates to pd.Timestamp\n",
    "        end_date = pd.Timestamp(end_date)\n",
    "    \n",
    "        mask = (sensor_readings['created_at'] > start_date) & (sensor_readings['created_at'] <= end_date) #mask for date range\n",
    "        masked_sensor_readings = sensor_readings.loc[mask]\n",
    "\n",
    "        try:\n",
    "            #First, find all possible violation-hours\n",
    "            ##We need to index as datetimeindex in order to use the .between_time method\n",
    "            sensor_readings.set_index(pd.DatetimeIndex(sensor_readings['created_at']), inplace = True)\n",
    "        \n",
    "            ##This returns the a list of day and night readings\n",
    "            day_readings = sensor_readings.between_time(start_time='06:00', end_time='22:00')\n",
    "            night_readings = sensor_readings.between_time(start_time='22:00', end_time='6:00')\n",
    "\n",
    "            ##Now, we count how many rows are violations and divide by total possible violation hours\n",
    "            #For day, if outdoor_temp < 55\n",
    "            day_total_violable_hours = len(day_readings.loc[day_readings['outdoor_temp'] < 55])\n",
    "            day_actual_violation_hours = len(day_readings.loc[day_readings['violation'] == True])\n",
    "            #For night, if outdoor_temp < 40\n",
    "            night_total_violable_hours = len(night_readings.loc[night_readings['outdoor_temp'] < 40])\n",
    "            night_actual_violation_hours = len(night_readings.loc[night_readings['violation'] == True])\n",
    "\n",
    "            #Calculate percentage\n",
    "            try:\n",
    "                violation_percentage = float(day_actual_violation_hours + night_actual_violation_hours)/float(day_total_violable_hours + night_total_violable_hours)\n",
    "            except ZeroDivisionError:\n",
    "                return \"No violations in this range.\"\n",
    "                \n",
    "            return violation_percentage #violationpercentage\n",
    "        \n",
    "        except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def violation_report():\n",
    "    unique_sensors = readings['sensor_id'].unique()\n",
    "    report = []\n",
    "    for ids in unique_sensors:\n",
    "         report.append(\"Sensor ID: {0}, Violation Percentage: {1}\".format(ids, violation_percentages(readings, '2016-01-01', '2016-02-07', ids)))\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function creates a simulated dataset of readings.\n",
    "def simulate_data(start_date, end_date, polling_rate, sensor_id): #polling_rate in minutes\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "\n",
    "    #how many hours between the two dates:\n",
    "    timedelta_in_seconds = end_date-start_date\n",
    "    total_number_of_hours = timedelta_in_seconds.total_seconds()/(polling_rate*60)\n",
    "    \n",
    "    #Create an empty pandas dataframe\n",
    "    index = xrange(1,int(total_number_of_hours)+1)\n",
    "    columns = ['created_at', 'sensor_id']\n",
    "    simulated_readings = pd.DataFrame(index = index, columns = columns)\n",
    "    simulated_readings.loc[:,'sensor_id'] = sensor_id\n",
    "    \n",
    "    #Populate it with columns of 'create_at' dates\n",
    "    time_counter = start_date\n",
    "    for i in simulated_readings.index.values:\n",
    "        simulated_readings.loc[i,'created_at'] = time_counter\n",
    "        time_counter = time_counter + pd.Timedelta('00:%s:00' % polling_rate)\n",
    "   \n",
    "    return simulated_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function generates a report; we might want to make this a cron job.\n",
    "def generate_report(start_date, end_date):\n",
    "    report = []\n",
    "    sensor_ids = readings.sensor_id.unique()\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "    for ids in sensor_ids:\n",
    "        temp = sensor_down(readings, start_date, end_date, ids)\n",
    "        if temp != None:\n",
    "            report.append(temp)\n",
    "        else:\n",
    "            pass\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7389025019316477"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "report = generate_report('2016-02-01','2016-02-07')\n",
    "header =['sensorID', 'status', 'Percentage of uptime in daterange', 'FirstName', 'LastName' , 'userID']\n",
    "\n",
    "toc = time.clock()\n",
    "toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserID\n",
      "SensorID\n",
      "Outside90\n",
      "Inside40\n",
      "InsideColderOutside\n"
     ]
    }
   ],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "report = dirty_data(readings)\n",
    "header = ['UserID', 'SensorID', 'Outside90', 'Inside40', 'InsideColderOutside']\n",
    "\n",
    "toc = time.clock()\n",
    "toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvoutput = open('sensors.csv', 'wb')\n",
    "writer = csv.writer(csvoutput)\n",
    "writer.writerow(header)\n",
    "for i in report:\n",
    "    writer.writerow(i)\n",
    "csvoutput.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report.to_csv('dirtydata.csv', index = False, na_rep=\"Not Currently Assigned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
